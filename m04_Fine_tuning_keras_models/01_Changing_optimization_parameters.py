# Import the SGD optimizer
from keras.optimizers import SGD

# Create list of learning rates: lr_to_test
lr_to_test = [.000001, 0.01, 1]

# Loop over learning rates
for lr in lr_to_test:
    print('\n\nTesting model with learning rate: %f\n'%lr )
    
    # Build new model to test, unaffected by previous models
    model = get_new_model()
    
    # Create SGD optimizer with specified learning rate: my_optimizer
    my_optimizer = SGD(lr=lr)
    
    # Compile the model
    model.compile(optimizer=my_optimizer, loss='categorical_crossentropy')
    
    # Fit the model
    model.fit(predictors, target)
    
    
#QUESTIONS
'''  It's time to get your hands dirty with optimization.
You'll now try optimizing a model at a very low learning rate, 
a very high learning rate, and a "just right" learning rate.
You'll want to look at the results after running this exercise, 
remembering that a low value for the loss function is good.

For these exercises, we've pre-loaded the predictors and target 
values from your previous classification models (predicting who would survive on the Titanic). 
You'll want the optimization to start from scratch every time you change the learning rate,
to give a fair comparison of how each learning rate did in your results.
So we have created a function get_new_model() that creates an unoptimized model to optimize. '''
